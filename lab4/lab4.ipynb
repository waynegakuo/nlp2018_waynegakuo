{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# Train classifier while normalizing text\n",
    "################################################################################################################\n",
    "\n",
    "def train():\n",
    "    # setting the data-set into a data-frame for easy management and manipulation using pandas\n",
    "    doc = pd.read_csv(\"amazon_cells_labelled.txt\", sep='\\t', names=['review', 'sentiment'])\n",
    "    # print(doc.tail())\n",
    "\n",
    "    # stop words such as 'a', 'is', 'are' are not significant to the corpus for analysis and therefore\n",
    "    # are stripped from the data set\n",
    "    wordset = set(stopwords.words('english'))\n",
    "\n",
    "    # Transforms text to feature vectors that can be used as input to estimator.\n",
    "    v = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words=wordset)\n",
    "\n",
    "    class_categ = doc.sentiment  # positive and negative classes\n",
    "\n",
    "    token = v.fit_transform(doc.review)  # tokenizing the reviews provided in the data-set\n",
    "\n",
    "    # print(class_categ.shape)  # number of observations/reviews\n",
    "    # print(token.shape)  # number of unique words after tokenizing\n",
    "\n",
    "    # Splits the class_categ and token arrays into random train and test subsets\n",
    "    token_train, token_test, class_train, class_test = train_test_split(token, class_categ, random_state=40)\n",
    "\n",
    "    # training the naive bayes classifier\n",
    "    naive_train = naive_bayes.MultinomialNB()\n",
    "    naive_train.fit(token_train, class_train)\n",
    "\n",
    "    return naive_train, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# Train classifier without normalizing text\n",
    "################################################################################################################\n",
    "\n",
    "def train_u():\n",
    "    # Not normalizing/tokenizing text\n",
    "    vu = TfidfVectorizer(use_idf=False, lowercase=False)\n",
    "\n",
    "    # setting the data-set into a data-frame for easy management and manipulation using pandas\n",
    "    doc = pd.read_csv(\"amazon_cells_labelled.txt\", sep='\\t', names=['review', 'sentiment'])\n",
    "\n",
    "    class_categ_u = doc.sentiment  # positive and negative classes\n",
    "    token_u = vu.fit_transform(doc.review)\n",
    "\n",
    "    # Splits the class_categ and token arrays into random train and test subsets\n",
    "    token_u_train, token_u_test, class_u_train, class_u_test = train_test_split(token_u, class_categ_u, random_state=40)\n",
    "\n",
    "    # training the naive bayes classifier\n",
    "    naive_train_u = naive_bayes.MultinomialNB()\n",
    "    naive_train_u.fit(token_u_train, class_u_train)\n",
    "\n",
    "    return naive_train_u, vu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "# Testing with normalized data\n",
    "###################################################################################################################\n",
    "\n",
    "def nb(cl, mod, test_file):\n",
    "    naive_train, v = train()\n",
    "    file = open(test_file, \"r\")\n",
    "    predict_array = []  # initialize array to contain classifier results\n",
    "    for line in file:\n",
    "        # treating each line by putting them into an array using an inbuilt panda function\n",
    "        movie_review_arr = pd.np.array([line])\n",
    "        movie_vect = v.transform(movie_review_arr)\n",
    "        class_placed = naive_train.predict(movie_vect)\n",
    "\n",
    "        # putting the classification results into an array\n",
    "        predict_array.append(class_placed)\n",
    "\n",
    "    f = open(\"nb-n.txt\", \"w\")\n",
    "\n",
    "    # writing the results into a text file\n",
    "    for item in predict_array:\n",
    "        f.write(str(item) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "# Testing with not normalized data\n",
    "###################################################################################################################\n",
    "\n",
    "def nb_u(cl, mod, test_file):\n",
    "    naive_train_u, vu = train_u()\n",
    "    file = open(test_file, \"r\")\n",
    "    predict_array_u = []  # initialize array to contain classifier results\n",
    "    for line in file:\n",
    "        # treating each line by putting them into an array using an inbuilt panda function\n",
    "        movie_review_arr = pd.np.array([line])\n",
    "        movie_vect = vu.transform(movie_review_arr)\n",
    "        class_placed = naive_train_u.predict(movie_vect)\n",
    "\n",
    "        # putting the classification results into an array\n",
    "        predict_array_u.append(class_placed)\n",
    "\n",
    "    f = open(\"nb-u.txt\", \"w\")\n",
    "\n",
    "    # writing the results into a text file\n",
    "    for item in predict_array_u:\n",
    "        f.write(str(item) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9351300b4be2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# accepting arguments from the command line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mnb_u\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Execute \n",
    "\n",
    "# accepting arguments from the command line\n",
    "nb(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "nb_u(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "\n",
    "if sys.argv[1] == \"nb\" and sys.argv[2] == \"n\":\n",
    "    cl = sys.argv[1]\n",
    "    mod = sys.argv[2]\n",
    "    test_file = sys.argv[3]\n",
    "    print(\"Naive Bayes Classifier with Normalized Data\")\n",
    "    nb(cl, mod, test_file)\n",
    "\n",
    "elif sys.argv[1] == \"nb\" and sys.argv[2] == \"u\":\n",
    "    cl = sys.argv[1]\n",
    "    mod = sys.argv[2]\n",
    "    test_file = sys.argv[3]\n",
    "    print(\"Naive Bayes Classifier Without Normalized Data\")\n",
    "    nb_u(cl, mod, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
